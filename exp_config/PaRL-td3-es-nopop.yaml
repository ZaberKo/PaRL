# ========= rllib config =========
framework: torch


placement_strategy: PACK

log_level: WARN
# log_level: INFO

# ============ env config ===========
env: HalfCheetah-v3
 
horizon: null
soft_horizon: false

# ============== target config==========
num_workers: 11 # number of workers for target policy
num_gpus: 0.1

batch_mode: complete_episodes
rollout_fragment_length: 1
episodes_per_worker: 1
explore: true
exploration_config: # default is GaussianNoise
  random_timesteps: 1000 # total_random_timesteps=11*1000

# ============= pop actors config ==============
pop_size: 0 # number of pop_workers
pop_config:
  explore: false
  num_envs_per_worker: 1
  batch_mode: complete_episodes
  rollout_fragment_length: 1
  exploration_config:
    random_timesteps: 0

# ========== ea config ==============
evolver_algo: es
ea_config:
  # CEM hyperparam
  # elite_fraction: 0.5
  # noise_decay_coeff: 0.95
  # noise_init: !!float 1e-3
  # noise_end: !!float 1e-5
  # GA
  # migration_freq: 10
  # migration_start: 0
  # elite_fraction: 0.2
  # crossover_prob: 0
  # mutation_prob: 0.9
  # NES
  # noise_stdev: 0.01
  # step_size: 0.01
  # es_optim: sgd
  # target_step_size: 0.5
  # pop_tau: 0.1
  # ES
  noise_stdev: 0.02
  parent_ratio: 0.5


# ========= evaluation worker config ==========
evaluation_num_workers: 10
evaluation_interval: 2
evaluation_duration: 10
evaluation_duration_unit: episodes
evaluation_config:
  explore: false
  num_envs_per_worker: 1
  # batch_mode: complete_episodes
  # rollout_fragment_length: 1
  # num_gpus: 0.1
  # num_gpus_per_worker: 0.01
# ============ training hyperparameter config =============
train_batch_size: &train_batch_size 256

actor_lr: !!float 3e-4
critic_lr: !!float 3e-4

# lr_schedule: [[0, !!float 3e-4],[50000000, !!float 1e-8]]
gamma: 0.99
tau: 0.005

policy_delay: 2


# ======== training process config =============
min_train_timesteps_per_iteration: 0

num_multi_gpu_tower_stacks: 16
learner_queue_size: 128
num_data_load_threads: 8
batch_bulk: 1

target_network_update_freq: 1 # unit: iteration


# ========= replay buffer config =============
replay_buffer_config:
  type: MultiAgentReplayBuffer
  capacity: !!int 1000000
  learning_starts: 10000
  # num_replay_buffer_shards: 1

compress_observations: false # also effect rollout transmission

# ============= checkpoint & stopper config ===============
tuner_config:
  num_samples: 5
  checkpoint_freq: 0 # 0: disable
  keep_checkpoints_num: null
  stopper:
    num_env_steps_trained: 256000000 # 256*1e6